%% intro.tex
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}

\subsection{Hierarchical}

The current model of the world-wide web has a hierarchical topology. Users typically
connect to the internet via an Internet Service Provider (ISP).
This gives Internet Service Providers the ability to analyse and block traffic which
passes through them. This ability can be used for censorship of the users.
Internet Service Providers typically use this ability to prevent their users from
accessing certain web-sites.

\subsection{Censorship}

Web censorship is often put in place by Internet Service Providers in response
to pressure or legal threats from organisations such as governments or large media establishments.
It is often the case that these organisations that impose this censorship on web
users act with their own best interests in mind, and not always that of the users. This is
particularly true in the case of governments blocking external media sources in
an effort to maintain political or social goals that could be interfered with
by media from other countries. Research done by the OpenNet Institute
shows that internet censorship is done for political, social, and conflict/security
reasons ~\cite{opennet}. Since the validity of this type of censorship is subjective, we argue that
web censorship of this nature is not justifiable and that the web should be free
from this censorship.

It is not the case, however, that censorship is universally bad. In some cases,
censorship is a viable means of removing harmful content from the internet.
However, it is difficult for any third party to accurately define what content
can be deemed objectively harmful for the users of a system. Therefore, we argue
that censorship should be decided only by community consensus by the users of a system.

\subsection{User Privacy}

Since all user traffic passes through them, Internet Service Providers also have
the ability to record and analyse user browsing habits. By analysing the destination
IP address of web-browsing traffic, ISPs can identify web-sites
that their users are visiting even when the content contained within the request is
encrypted.

SSL certificates, used for encryption when serving a web-site over HTTPS rely on a
centralised certificate-validation system. Due to the expense of SSL certificates,
much web traffic on the Internet uses the un-encrypted HTTP protocol rather than
encrypted HTTPS. This means that users web traffic is transmitted un-encrypted: without
HTTPS, user passwords and web content can be read by malicious users on a shared network.

\subsection{Centralisation of hosting}

Web-site hosting is usually centralised. It is typical for a web-site to be served by
one single web server which serves content to all of that web-site's users. This leads
to a single-point-of-failure in web serving. Hardware or software failure at this server
will lead to a loss of service for this web-site. Attackers that can take this single web-server
down will result in a global failure of this web-site. This is often done through an attack
called a Distributed Denial of Service (DDoS).

\section{Goals}

This project aims to design an alternate model for web content delivery on the internet.
The model should aim to address privacy and anonymity issues in the current model.
The project goals are as follows:

\begin{enumerate}

    \item{\textbf{Decentralisation} \\
The system should aim to avoid the centralisation that web-site hosting currently uses.
Decentralising the system means that no resources will be controlled by one centralised
authority, putting the system in the hands of the users themselves.
        }

    \item{\textbf{Author Anonymity} \\
Users who wish to publish content on this system must be able to do so anonymously.
Web content published to the system must not be traceable to the user that created it.
Users, then, can not hold their own content: it must be co-operatively held by other
users of the system in exchange for holding their content.
        }

    \item{\textbf{Reader Privacy} \\
Users who wish to consume content on this system must be able to do so with privacy.
It should be impossible for malicious users who intercept traffic on this system
(either via a shared connection or by being their ISP,  etc) should not be able to
determine what content a user is consuming.
        }

    \item{\textbf{Community-Driven Censorship} \\
Censorship is often controlled by large, centralised powers such as governments.
Due to this, it is not always the case that web users are in control of what content
has been made unavailable to users of the system. Users of the system should be
in control of what content is or is not acceptable.
        }

    \item{\textbf{Increased Reliability and Redundancy} \\
Web content stored on the system should not be reliant on any one single node.
Content should be spread across nodes to ensure that it remains on the system despite
single users leaving.
        }

    \item{\textbf{Secure Data Storage} \\
Users can not all hold their own data because this would violate a user's anonymity.
Other users must hold it for them. As a result, we must ensure that the users storing
your web content can neither tamper with it nor read its contents. For them to do so could
possibly breach the authorâ€™s privacy. This necessitates secure data storage.
Furthermore, users that hold data for other users must not be able to be held responsible
for the contents that other users have requested them to store. Therefore, it must be
impossible for a user to know the contents of the data that they store for other users
and what this data represents.
        }
\end{enumerate}
